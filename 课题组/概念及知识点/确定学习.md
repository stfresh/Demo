Wang等学者提出，用于研究控制系统中未知动态的经验知识获取、存储和再利用的相关问题。
><font  
color="red"  
size="3">引理</font>：对于属于紧集$\Omega_z$内的任意周期或类周期轨迹$Z(t)\in \Omega_z\subseteq R^q$。合理安排$RBF$神经网络$W_TS(Z)$的神经元节点，使节点完全覆盖住紧集$\Omega_z$，那么$RBF$神经网络中在轨迹$Z(t)$周围的回归子向量$S_\zeta(Z)$满足持续激励条件。

结合该引理以及[[线性时变系统的指数稳定性]]，确定学习理论证明了沿着周期轨迹$Z(t)$的$RBF$神经网络的权值能够指数收敛到理想的常值权值向量，数学表达式如下：$$\bar W=mean_{t\in [t_a,t_b]}\hat W(t)=\frac{1}{t_b-t_a}\int_{t_a}^{t_b}\hat W(s)ds$$
其中$\left[t_a,t_b\right]$表示$RBF$神经网络权值收敛到理想常数权值后的一段时间区间。$\bar W$是收敛后的常值权值向量。$\hat W(t)$是理想权值$W^*$的估计向量。利用收敛后的常值权值向量$\bar W$，当再次遇到相同或类似的控制任务时，可以利用保存的常值权值向量$\bar W$对系统未知动态进行辨识，也即表示为：$$f(Z)=\bar W^TS(Z)+\delta$$
其中$\delta$为逼近误差。
